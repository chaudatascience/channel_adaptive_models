batch_strategy: random_instance 
resume_train: False
resume_model: ~  # model_5.pt  # used when `resume_train` is True
use_amp: False   #  mixed precision training
checkpoints: ../STORE/adaptive_interface/checkpoints
save_model: False # save model after each epoch to checkpoints
clip_grad_norm: ~   # or ~ to NOT use
batch_size: 128 # batch_size vs. num_classes?
num_epochs: 15
verbose_batches: 50
seed: ~  # if ~, will generate a random number for the seed
debug: False
adaptive_interface_epochs: 0 # set to 0 to disable
adaptive_interface_lr: ~  # if ~, will use  100x of the fine-tuning lr
swa: False
swad: True
swa_lr: 0.05
swa_start: 5
miro: False
miro_lr_mult: 10.0
miro_ld: 0.01
tps_prob: 0.0   ## TPS transformation. 0 means disable. To use, set a value in (0, 1]
ssl: False  ## self-supervised loss
ssl_lambda: 0.0  ## lambda to balance the self-supervised loss with the main loss